{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and implement a multiclass classification deep neural network model to classify between Positive/Extremely Positive/Negative/Extremely Negative/Neutral sentiments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the data manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Data Preprocessing libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import re\n",
        "import contractions\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Importing the model building libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, Dense, Dropout, GlobalMaxPooling1D, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from wordcloud import WordCloud\n",
        "# Importing the evaluation libraries\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "O5RcxwQUku6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Load the Dataset\n"
      ],
      "metadata": {
        "id": "xIa9LlhMHj5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "train_data = pd.read_csv('/content/corona_nlp.csv',encoding='latin-1')  # Adjust path as needed"
      ],
      "metadata": {
        "id": "6qmR5Vo_tbVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check for Missing Values"
      ],
      "metadata": {
        "id": "hzQS91rfJLNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "JF3xCD9qJN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize the sentiment column values\n"
      ],
      "metadata": {
        "id": "nra2K6EPHosw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "5ksnP-I2Fitd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize top 10 Countries that had the highest tweets using countplot (Tweet count vs Location)\n"
      ],
      "metadata": {
        "id": "_zc6AUq9Hry8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Location'].value_counts().head(10)"
      ],
      "metadata": {
        "id": "M_fUPMJzGl8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Plotting Pie Chart for the Sentiments in percentage\n"
      ],
      "metadata": {
        "id": "GUIM_P-VHuzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Sentiment'].value_counts().plot(kind='pie', autopct='%1.0f%%')"
      ],
      "metadata": {
        "id": "s8oRZOYDHAVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WordCloud for the Tweets/Text\n",
        "\n",
        "    * Visualize the most commonly used words in each sentiment using wordcloud\n",
        "    * Refer to the following [link](https://medium.com/analytics-vidhya/word-cloud-a-text-visualization-tool-fb7348fbf502) for Word Cloud: A Text Visualization tool\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSvzz5z6H8kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in train_data['Sentiment'].unique():\n",
        "    all_tweets = ' '.join(train_data[train_data['Sentiment'] == sent]['OriginalTweet'].to_list())\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_tweets)\n",
        "    print(f\"Word Cloud for Sentiment: {sent}\")\n",
        "    plt.figure(figsize=(10, 5))  # Set figure size\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")  # Turn off axis\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vq7__byEHabv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Stage 2**: Data Pre-Processing  \n",
        "####  Clean and Transform the data into a specified format\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data['OriginalTweet']\n",
        "y = train_data['Sentiment']\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "Xl4w05HMe0ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "HsMA2mI1efz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_data['OriginalTweet']"
      ],
      "metadata": {
        "id": "3wSdK3ozfqfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.pie(pd.Series(y_train).value_counts(), labels=pd.Series(y_train).value_counts().index, autopct='%1.2f%%')\n",
        "plt.title(\"TRAIN\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.pie(pd.Series(y_val).value_counts(), labels=pd.Series(y_val).value_counts().index, autopct='%1.2f%%')\n",
        "plt.title(\"VALIDATION\")"
      ],
      "metadata": {
        "id": "zT-ZnTaNfJw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_val = label_encoder.transform(y_val)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)"
      ],
      "metadata": {
        "id": "RlPsC9Y_fSh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unnecessary_elements(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'(covid[-_]?19|covid2019|covid[-_]?2019|corona[-_]?virus|corona|covid)', 'covid', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\t', ' ', text)\n",
        "    text = re.sub(r'\\r', ' ', text)\n",
        "    text = re.sub(r'â|â’', \"'\", text)\n",
        "    text = re.sub(r'\\x92|\\xa0|\\x85|\\x95', '', text)\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "X_train = np.array([remove_unnecessary_elements(text) for text in X_train])\n",
        "X_val = np.array([remove_unnecessary_elements(text) for text in X_val])\n",
        "X_test = np.array([remove_unnecessary_elements(text) for text in X_test])"
      ],
      "metadata": {
        "id": "Q8h3Hc_Ifjvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing without limiting vocabulary size, to get word counts\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "word_counts = len(tokenizer.word_index) + 1  # vocabulary size\n",
        "\n",
        "print(\"Numbers of unique words present:\", word_counts)"
      ],
      "metadata": {
        "id": "-eTHMpLnf36m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "maxlen = max([len(seq) for seq in train_sequences])\n",
        "print(\"Maximum length of all sequences:\", maxlen)\n",
        "\n",
        "padded_train_sequences = pad_sequences(train_sequences, maxlen=maxlen, padding='post')\n",
        "print(\"Padded TRAINING Sequences Shape:\", padded_train_sequences.shape)"
      ],
      "metadata": {
        "id": "atgk4zWgf7HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longest_sequence_index = np.argmax([len(seq) for seq in train_sequences])\n",
        "\n",
        "# Get the longest sequence and its corresponding original sentence\n",
        "longest_sequence = train_sequences[longest_sequence_index]\n",
        "longest_sentence = X_train[longest_sequence_index]\n",
        "\n",
        "print(longest_sentence)"
      ],
      "metadata": {
        "id": "E6EX8Gw8f9Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Validation set\n",
        "val_sequences = tokenizer.texts_to_sequences(X_val)\n",
        "padded_val_sequences = pad_sequences(val_sequences, maxlen=maxlen, padding='post')\n",
        "\n",
        "print(\"Padded VALIDATION Sequences Shape:\", padded_val_sequences.shape)"
      ],
      "metadata": {
        "id": "nGbKOMSkgA9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For test set\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "padded_test_sequences = pad_sequences(test_sequences, maxlen=maxlen, padding='post')\n",
        "\n",
        "print(\"Padded TEST Sequences Shape:\", padded_test_sequences.shape)"
      ],
      "metadata": {
        "id": "mV435QvTgDNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word]\n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "qJG1Rs-2gGdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "2MxkIx_SgPHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix = create_embedding_matrix('glove.6B.100d.txt', tokenizer.word_index, embedding_dim)"
      ],
      "metadata": {
        "id": "hE42Et3pgIkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "print(f\"Percent of vocabulary covered: {round(nonzero_elements/word_counts*100, 2)}%\")"
      ],
      "metadata": {
        "id": "KhZ3eyfRhHvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the words that are not covered by GloVe\n",
        "not_covered_words = []\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    if np.count_nonzero(embedding_matrix[idx]) == 0:  # If the embedding vector is all zeros\n",
        "        not_covered_words.append(word)\n",
        "\n",
        "# Print some of the words that are not covered\n",
        "print(f\"Total uncovered words: {len(not_covered_words)}\")\n",
        "print(\"Sample of uncovered words:\", not_covered_words[:50])"
      ],
      "metadata": {
        "id": "tXGeNnClhJF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded_train_sequences)  # training feature\n",
        "print(y_train)  # training target"
      ],
      "metadata": {
        "id": "A_7OZs0QhNXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   **Stage 4**: Build model"
      ],
      "metadata": {
        "id": "fBAgSzpkWaoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(word_counts,\n",
        "                    embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=maxlen,\n",
        "                    trainable=True))\n",
        "\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "\n",
        "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Bidirectional(LSTM(units=32, return_sequences=False)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(24, activation='relu', kernel_regularizer=l2(0.05)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.05)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "metadata": {
        "id": "WjY7HXBShPDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.build((padded_train_sequences.shape))"
      ],
      "metadata": {
        "id": "30DKaaEJhTW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "n9oyQU3shUOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=True, dpi=90)"
      ],
      "metadata": {
        "id": "yAHWaAszhX2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `y_train_original` holds the non-one-hot encoded original sentiment labels\n",
        "y_train_original = np.argmax(y_train, axis=1)  # Converting one-hot encoded y_train back to label form\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_original), y=y_train_original)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "num_epochs = 200\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "-fGihJpLhc28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.0002, clipnorm=1.0),\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "aR2dwtkRhfVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(padded_train_sequences,\n",
        "                    y_train,\n",
        "                    validation_data=(padded_val_sequences, y_val),\n",
        "                    epochs=num_epochs,\n",
        "                    class_weight=class_weights,\n",
        "                    callbacks =[reduce_lr, early_stop, checkpoint],\n",
        "                    batch_size=32,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "j5HA7UnvhgUd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}